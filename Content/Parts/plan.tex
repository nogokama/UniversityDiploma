%!TEX root=../main.tex

\section*{Аннотация}

\vspace{-0.5cm}
Алгоритмы планирования задач на вычислительных кластерах являются предметом активных исследований. Проведение таких исследований на настоящем кластере долго и дорого, поэтому для сравнения различных гипотез используются симуляторы. Качество получаемых результатов во многом зависит от точности используемой модели. Большинство современных симуляторов используют простые модели, которых не всегда достаточно. Эта работа сосредоточена на разработке симулятора вычислительного кластера на базе фреймворка DSLab, который позволит очень точно и гибко моделировать сценарии вычислительной нагрузки с использованием примитивов асинхронного программирования. \texttt{DSLab} предоставляет массу возможностей как для написания новых алгоритмов, так и для использования готовых модулей различных компонент распределенной системы. Таким образом, новый симулятор получит основные преимущества DSlab, которые сделают его еще более привлекательным для исследователей сложных алгоритмов.

\vspace{-0.5cm}
\section*{Ключевые слова}

Распределенные системы, вычислительный кластер, алгоритмы планирования, симуляция, DSLab
\vspace{-0.5cm}
\section*{Annotation}
\vspace{-0.5cm}
Algorithms for scheduling tasks on compute clusters are the subject of active research. It is quite expensive to conduct such studies using real cluster, so simulators are used to compare different hypotheses. The quality of the results obtained largely depends on the accuracy of the model used. Most modern simulators use simple compute models, which are not always enough. Our work is focused on developing a simulator of a compute cluster based on the DSLab framework, which will allow very accurate and flexible modeling of computational load scenarios using asynchronous programming primitives. DSLab provides a lot of opportunities both for writing new algorithms and using ready-made modules of various distributed system components. Thus, the new simulator will receive the basic advantages of DSLab, which will make it even more attractive for researchers of complex algorithms.


\vspace{-0.5cm}
\section*{Keywords}
Distributed systems, compute cluster, scheduling algorithms, simulations, DSLab 

\newpage 

\section{Введение}

Вычислительные кластеры, состоящие из набора серверов с различными ресурсами (процессор, память, диск), широко применяются для проведения сложных расчетов и обработки больших объемов данных в науке и бизнесе. На большом промышленном кластере одновременно могут выполняться тысячи заданий (batch jobs) различных пользователей, а часть заданий может находиться в очереди в ожидании выделения ресурсов. Запуском заданий и распределением ресурсов между ними управляет менеджер кластера или планировщик. В основе работы менеджера кластера лежат алгоритмы планирования задания (job scheduling), основной целью которых является максимально эффективное использование ресурсов кластера. Данные алгоритмы являются предметом активных исследований. Проводить такие исследования на реальном кластере долго и дорого, поэтому необходима компьютерная модель (симулятор), позволяющая быстро проверить гипотезу или провести сравнительное тестирование разных алгоритмов на некоторой истории нагрузки кластера. Подобный симулятор также может использоваться в учебном процессе для знакомства студентов с такими системами и возникающими в них задачами.

\subsection{Актуальность и значимость}

Алгоритм планирования задач -- ключевая часть архитектуры всех вычислительных кластеров. Поиск оптимальных алгоритмов очень актуален, поскольку повышение утилизации имеющихся ресурсов позволит снизить затраты на оборудование кластера и ускорить его работу. Существующие симуляторы в основном предлагают простые модели задач или углубляются в конкретную область (например, глубинное обучение). 

Новый симулятор, разработанный на базе фреймворка \texttt{DSLab} позволит исследователям подбирать алгоритмы под конкретные сценарии нагрузки, используемые в их кластере. Таким образом, симулятор позволит более точно предсказывать поведение настоящего кластера.  

\subsection{Цели и задачи ВКР}

Целью ВКР является разработка симулятора вычислительного кластера на базе фреймворка DSLab. 

Для достижения этой цели были поставлены следующие задачи: 
\begin{enumerate}
\item  Изучить литературу по теме и существующие симуляторы, подготовить обзор с анализом их преимуществ и недостатков.
\item Поэтапно реализовать компоненты симулятора, покрыть их тестами и снабдить комментариями.
\item Подготовить и провести эксперименты, демонстрирующие работоспособность симулятора и достижение всех требований.
\item Подготовить документацию пользователя.
\end{enumerate}

\subsection{Основные результаты работы}

В результате ВКР был разработан эффективный и многофункциональный симулятор вычислительного кластера на базе фреймворка \texttt{DSLab}, были реализованны несколько популярных алгоритмов планирования задач и проведены тесты на данных трейсов различных компаний, а также синтетических данных. 


\subsection{Структура работы}

Дальнейшая структура работы следующая: 
\begin{itemize}
    \item Глава \ref{sec:lit-review} содержит обзор литературы и существующих симуляторов.
    \item Глава \ref{sec:dslab} описывает фреймворк DSLab и его асинхронное расширение, на базе которого был разработан симулятор.
    \item Глава \ref{sec:main-impl} описывает реализацию симулятора, основные компоненты и используемые модели.
    \item Глава \ref{sec:experiments} содержит описание проведенных экспериментов на разработанном симуляторе и их анализ.
    \item Глава \ref{sec:conclusion} подводит итоги работы и предлагает дальнейшие направления разработки и улучшения симулятора.
\end{itemize} 

\section{Обзор литературы}\label{sec:lit-review}

\subsection{Вычислительная модель}

Для моделирования работы кластера и алгоритмов планирования требуется модель рабочей нагрузки. Standard Workload Format(SWF)\cite{standard-workload-format} широко используется в литературе и поддерживается многими симуляторами. Однако, SWF предоставляет довольно мало информации о рабочей нагрузке. Ключевая информация о единице рабочей нагрузки <<Job>> включает время прибытия, время выполнения, приоритет и потребность в ресурсах. Единственное предположение, которое мы можем сделать относительно рабочей нагрузки заключается в том, что <<Job>> потребляет все заданные ресурсы во время выполнения. Наиболее популярные трейсы рабочей нагрузки представлены в аналогичном формате (например, Google trace\cite{google-clusterdata}).

Однако, существует и более сложный подход к моделированию задач на кластере, при котором время выполнения задачи рассчитывается на основе структуры рабочей нагрузки и доступных ресурсов. Основное преимущество этого подхода заключается в том, что больше деталей задачи могут быть учтены и использованы при планировании. Кроме того, появляется возможность с большей точностью рассчитать загрузку кластера и поведение его компонент. Основной недостаток -- для такого моделирования довольно сложно найти данные из открытых источников, поскольку информация о детальной структуре задач обычно конфиденциальна и не публикуется.

\subsection{Аналоги}

Существующие симуляторы относятся к одной из двух категорий: симуляторы, созданные с нуля, и основанные на готовой платформе. Другой способ группировки симуляторов -- по типу рабочей нагрузки, которую они поддерживают: симуляторы, которые работают с SWF или аналогичными форматами, и поддерживающие сложное описание рабочей нагрузки.

\subsubsection{AccaSim}

Характерным примером такого симулятора <<с нуля>> является AccaSim\cite{accasim-article}. Он основан на дискретно-событийном моделировании и содержит предопределенные алгоритмы планирования. Он довольно прост в использовании, но не поддерживает сложные конфигурации. В отличие от AccaSim, наш симулятор сможет моделировать произвольные сценарии рабочей нагрузки, которые будут определены пользователями. Кроме того, наш симулятор является модульным, поскольку основан на DSLab. Пользователи смогут подключать модули, написанные другими исследователями в рамках платформы DSLab, и использовать их в своих экспериментах. Например, DSLab предоставляет модели сети, которые могут точно симулировать различные топологии.

\subsubsection{BatSim}

Симулятор Batsim основан на SimGrid\cite{simgrid-article} и позволяет разработчикам интегрировать алгоритмы на нескольких языках на основе inter-process communication. Подробное сравнение SimGrid и DSLab можно найти в документации по DSLab\cite{dslab-analog-cmp}. Ключевое ограничение BatSim -- из возможных ресурсов кластера он поддерживает моделирование только <<вычисления>>, <<передачу данных>> и <<операции с распределенной файловой системой>>. Batsim предоставляет интерфейс для настройки пользовательских типов рабочей нагрузки, но они ограничены несколькими заданными паттернами и возможными последовательными комбинациями из них (см. рис. \ref{fig:batsim-profile}). 

\begin{figure}[h]
    \footnotesize  
\begin{jsoncode}
"jobs": [
  {"id":"simple", "subtime":1, "res": 4, "profile": "simple"},
],
"profiles": {
  /* workload profile definition */
  "simple": {
    "type": "parallel",
    "cpu": [5e6,  0,  0,  0],
    "com": [5e6,  0,  0,  0,
            5e6,5e6,  0,  0,
            5e6,5e6,  0,  0,
            5e6,5e6,5e6,  0]
  },
}
\end{jsoncode}
\caption{Пример описания нагрузки в BatSim}
\label{fig:batsim-profile}
\end{figure}

Так же из рис. \ref{fig:batsim-profile} можно увидеть, что потребность в ресурсах задачи выражается в количестве выделенных <<ресурсов>> (одно число -- количество серверов), и задача размещается на них целиком. Таким образом, существенно ограничивается планирование задач с разным профилем требований к ресурсам (задача упаковки). Стоит отметить, что каждый сервер имеет свою вычислительную мощность, а так же каким-то образом размещен в топологии сети, поэтому проблема размещения задач оптимальным образом на кластере не сводится к тривиальной. Необходимость учитывать топологию сети в такой модели -- сложная проблема планирования. Однако, проблема упаковки задач на несколько ресурсов в такой модели не рассматривается.

Наш симулятор расширит подход BatSim для определения произвольной рабочей нагрузки, но при этом предоставит больше возможностей для управления ресурсами (процессор, память, диск), а так же предоставит еще больше возможностей для описания сценария нагрузки с использованием примитивов асинхронного программирования.

\subsubsection{IRMaSim}

Дальнейшим развитием BatSim стал симулятор IRMaSim\cite{irmasim-article}. В нем отмечаются недостатки BatSim в точности моделирования вычислений и предоставляются намного более точные модели процессоров и добавляются различные модели оперативной памяти. В этом симуляторе возможно точно указать тим оперативной памяти и ее скорость, и все это будет учитываться в симуляции. Так же этот симулятор поддерживает сценарии работы с задачами глубинного обучения (Deep Reinforcement Learning). Однако, ключевое ограничение остается то же -- потребность задач в ресурсах указывается одним числом -- количеством <<ресурсов>> (серверов). По аналогии с BatSim, этот симулятор фокусируется на потреблении энегрии в процессе работы кластера и позволяет разрабатывать алгоритмы планирования, оптимизирующие энергопотребление. 

\subsubsection{Elastisim}

Симулятор Elastisim\cite{elastisim-article} также основан на SimGrid и ориентирован на гибкие рабочие нагрузки (malleable workloads). Это позволяет моделировать динамические изменения количества ресурсов, выделяемых на исполнение задач. Одна из важных особенностей Elastisim -- высокая точность моделирования задач глубинного обучения, для которых он и предназначен. Кроме этого, в Elastisim поддерживатеся моделирование передачи данных (I/O) по сети. Авторы обращают особое внимание на то, что операции I/O могут стать узким местом, поскольку объем передаваемых данных постоянно увеличивается. Выполнение каждой задачи состоит из нескольких этапов и может быть настроено с помощью файла JSON путем объединения предварительно определенного набора операций: CPU, GPU, операций I/O и дисковых операций. Это самый продвинутый симулятор из всех рассмотренных источников. Мы будем стремиться предоставить еще больше возможностей для подробного описания выполняемых задач, однако наш симулятор будет поддерживать более распространенную модель вычислений, при которой каждой задаче выделяется фиксированная аллокация ресурсов на время выполнения.

\subsection{Выводы}

Рассмотренные симуляторы решают разные задачи симуляции: симуляция упаковки задач на кластер в условиях нескольких ресурсов (AccaSim), и точная симуляция вычислительного процесса (BatSim, IRMaSim, Elastisim). При этом ни в одном симуляторе эти задачи не пересекаются. Наш симулятор разработан в более общей модели, что позволяет поддержать задачу упаковки и точное моделирование вычислений одновременно. Более того, одной из тем исследований планирования задач на кластерах является честность (в условиях, когда кластер делится между несколькими пользователями). Задача честности не рассматривалась в приведенных симуляторах, однако наш симулятор поддерживает и эту возможность.



\section{Фреймворк \texttt{DSLab}}\label{sec:dslab}

\subsection{Архитектура \texttt{DSLab}}


В качестве платформы для реализации симулятора выбран \texttt{DSLab} -- модульный фреймворк для моделирования и тестирования распределенных систем. Модули можно разделить на 3 группы: базовые, универсальные и специализированные (см рисунок \ref{fig:dslab_arc}). За основу взят подход дискретно-событийного моделирования, при котором компоненты симуляции обмениваются событиями через базовый низкоуровневый модуль \texttt{dslab-core}. Соответственно, пользовательская логика каждого компонента состоит из обработки входящих событий от других компонент и отправки новых. Универсальные модули могут использовать исследователями соответствующих предметных областей, например, новый симулятор использует готовые модули вычислений, сети и диска (подробнее об этом в главе \ref{cluster-model}). 

\begin{figure}[H]
    \centering
\includegraphics[width=0.7\linewidth]{images/dslab_arc}
\caption{Архитектура \texttt{DSLab}}
\label{fig:dslab_arc}
\end{figure}

Описание архитектуры основано на официальной документации \cite{dslab-architecture}. Реализация всех модулей опубликована в GitHub-репозитории\cite{dslab-repo}.

\subsection{Асинхронное управление событиями}

В предыдущей работе была добавлена возможность управлять событиями \texttt{DSLab} асинхронно\cite{async-dslab}, что существенно расширило возможности написания компонент и сделало возможным ключевую особенность нового симулятора -- описание профиля нагрузки задач через асинхронные примитивы языка \texttt{Rust} (подробнее об этом в главе \ref{compute-model}). Ключевая особенность такого расширения -- возможность спрятать от пользователя явную работу с событиями, предоставив ему высокоуровневые асинхронные функции, в реализации которых зашито асинхронное ожидание внутренних событий. Например, при работе с записью на диск можно предоставить пользователю следующую асинхронную функцию: 
\begin{figure}[h]
    \footnotesize
    \begin{rustcode}
pub async fn write_data(&self, size: u64) -> Result<(), String> {
  let req_id = self.disk.write(size, /*requester=*/self.ctx.id());

  select! {
    _ = self.ctx.recv_event_by_key::<DataWriteCompleted>(req_id).fuse() => {
        Result::Ok(())
    }
    failed = self.ctx.recv_event_by_key::<DataWriteFailed>(req_id).fuse() => {
        Result::Err(failed.data.error)
    }
  }
}
    \end{rustcode}
    \caption{Пример асинхронной функции записи данных на диск.}
\end{figure}

В этом примере используется детализированное ожидание события по ключу. Исполнение симуляции прерывается до того момента, пока компонент (в данном случае -- вычислительный сервер, на котором запущена задача) не получит событие от собственного диска о том, что запись завершилась (либо успешно, либо с ошибкой -- для альтернативного ожидания используется макрос \texttt{select!} из библиотеки \texttt{futures}\cite{rust-futures}), после этого ядро симуляции продолжит исполнение и результат вернется пользователю.  

Таким образом, пользователю не нужно будет самостоятельно следить за событиями компонент и можно будет разделить инструкции выполнения задач от конкретных компонент, предоставив лишь <<сценарий работы>> в виде асинхронных функций. 

Точно таким же образом реализованы и другие примитивы: вычисления и  работа с сетью. 


\section{Реализация симулятора}\label{sec:main-impl}


\subsection{Архитектура симулятора}

По аналогии с самим фреймворком \texttt{DSLab} симулятор модульный, его архитектура представлена на рисунке \ref{fig:arch}.

\begin{figure}[h!]
    \centering 
    \includegraphics[width=\textwidth]{images/simulator_arc}
    \caption{Архитектура симулятора}
    \label{fig:arch}
\end{figure}

Симулятор может работать в нескольких режимах, принимая на вход несколько видов входных данных. Это может быть как готовый трейс нагрузки из открытых источников (например, Google Trace), так и пользовательский ввод задач, описанный в виде \texttt{YAML}-файла (подробнее в разделе \ref{sec:input-format-trace}).

Собирая информацию с компонентов \texttt{Proxy} и \texttt{Cluster} можно получить статистику о работе кластера и алгоритмов планирования. Например, насколько загружен кластер, насколько хорошо утилизируются его ресурсы. Такие выходные данные могут быть использованы для анализа работы алгоритмов и сравнения их эффективности. За это отвечает отдельный компонент -- \texttt{Monitoring}(подробнее в разделе \ref{sec:output-monitoring}).


\subsection{Модель кластера}\label{cluster-model}
Вычислительный кластер -- это набор отдельных серверов, которые могут быть связаны между собой моделируемой сетью (подробнее в разделе \ref{sec:network-model}). 
В симуляторе используются готовые модули DSLab, из которых собирается каждый сервер (см. таблицу \ref{tab:server_modules}). 
\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|c|}
    \hline
    \textbf{Название модуля} & \textbf{Используемый модуль} & \textbf{Обязательный} \\
    \hline
    \texttt{compute} & \texttt{dslab\_compute::multicore::Compute} & да \\
    \hline
    \texttt{network} & \texttt{dslab\_network::Network} & нет  \\
    \hline
    \texttt{disk} & \texttt{dslab\_storage::disk::Disk} & нет \\
    \hline
    \end{tabular}
    \caption{Модули каждого сервера в кластере}
    \label{tab:server_modules}
\end{table}

Все эти модули конфигурируются при создании кластера. В случае, если хост запланировано выходит из строя или начинает работу после начала симуляции, его модули все равно создаются сразу как компоненты симуляции \texttt{DSLab}. Это необходимо, поскольку каждый сервер и каждый модуль сервера является обработчиком событий в симуляции и должен быть соответствующим образом зарегистрирован.

\subsubsection{Модуль вычислений}

Для вычислений используется модуль \texttt{Compute} с фиксированными слотами -- <<аллокациями>>, которые состоят из некоторого количества ядер процессора и объема выделенной оперативной памяти. В текущей используемой модели аллокации не могут пересекаться, однако могут быть заняты не полностью, что скажется на утилизации кластера. Т.е. на данном этапе планировщик не имеет возможность выделить больше ресурсов чем есть на сервере в надежде, что запущенные задачи используют меньше, чем просят. 

Поскольку некоторые трейсы нагрузки не содержат точную информацию о конфигурации кластера, единицы измерения <<ядер процессора>> и размера оперативной памяти могут быть относительны. Например, в Google Trace\cite{google-clusterdata} размер оперативной памяти и количества ядер процессора указаны как доля от максимального значения в кластере. Аналогичным образом нормированы и требования задач к ресурсам. Такой политики компании придерживаются с целью не раскрывать реальные конфигурации своих серверов и требования к задачам. Поскольку модуль \texttt{Compute} принимает на вход целые числа (количество ядер и объем памяти), то при создании кластера в симуляции данные в трейсе доли умножаются на большие константы таким образом, чтобы получились целые числа и погрешность была небольшой (очевидно, что при этом теряются единицы измерения). Все константы задаются пользователем при конфигурации симуляции. 

Подробнее описание модуля \texttt{Compute} и примеры использования можно посмотреть в официальной документации \texttt{DSLab}\cite{dslab-compute-docs}.

\subsubsection{Модуль сети}\label{sec:network-model}

Модуль сети \texttt{Network} содержит несколько моделей на выбор, которыми можно воспользоваться в симуляции:
\begin{enumerate}
    \item \texttt{ConstantBandwidthNetworkModel} -- самая простая модель, которая предполагает, что все сервера в кластере соединены сетью с постоянной пропускной способностью. Нагрузка сети одними компонентами никак не влияет на другие. Подходит для простой и быстрой симуляции сетевого взаимодействия.
    \item \texttt{SharedBandwidthNetworkModel} -- модель, предполагающая наличие одного сетевого канала, общего для всех участников сети. Может быть достаточно точной в ситуации, когда все компоненты сети подключены к одному центральному коммутатору и пропускная способность сети много превосходит пропускную способность центрального коммутатора, являющегося узким местом. Однако, в реальности для такого случая можно применить модель \texttt{Star-Topology} и получить существенно большую точность с небольшим штрафом в производительности. Поэтому и эту модель используют с целью более быстрого моделирования сети в ущерб точности. 
    \item \texttt{TopologyAwareNetworkModel} -- гибко настраиваемая и достаточно точная модель сети с возможностью задания топологии. В симуляторе пользователь может самостоятельно настроить и передать симулятору сеть, которая наиболее точно отражает желаемую ситуацию. Однако, по умолчанию предоставляется вариант \texttt{Fat-Tree Topology}, который на практике чаще всего используется в датацентрах и кластерах, поскольку является эффективным и отказоустойчивым\cite{fat-tree-networks-article}.  Схематично топология сети \texttt{<<Fat-Tree>>} представлена на рисунке \ref{fig:fat_tree_topology}. Серверы объединяются в стойки (L1 Switch), которые связаны между собой коммутаторами (L2 Switch). Все пропускные способности и задержки между узлами, а также количество серверов в стойках и количество верхнеуровневых коммутаторов задаются пользователем при конфигурации симуляции.
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.8\linewidth]{images/fat_tree_topology}
        \caption{Пример топологии сети <<Fat-Tree>> на 6 серверов по 2 сервера в стойке}
        \label{fig:fat_tree_topology}
    \end{figure}
\end{enumerate}

Подробнее описание модуля \texttt{Network} и примеры использования можно посмотреть в официальной документации \texttt{DSLab}\cite{dslab-network-docs}.

\subsubsection{Модуль хранилища}

Модуль диска используется для симуляции операция чтения-записи в хранилище. Модель предоставляет возможность гибко настроить деградацию производительности диска в зависимости от количества параллельных операций (особенно актуально для HDD операций). Также модель предоставляет возможность реализовать пользовательский планировщик дисковых операций, но по умолчанию используется классический \texttt{<<FIFO>>}-планировщик (First-In-First-Out).  

Подробнее описание модуля \texttt{Storage} и примеры использования можно посмотреть в официальной документации \texttt{DSLab}\cite{dslab-storage-docs}.




\subsection{Вычислительная модель}\label{compute-model}

\subsubsection{Деление на Collection и Execution}\label{sec:execution-collection-model}

Поскольку симулятор может поддерживать несколько разных вычислительных моделей, для описания нагрузки был выбран обобщенный вариант: пользователь описывает задачи (\texttt{Execution}), которые непосредственно запускаются на кластере и потребляют ресурсы, и эти задачи могут быть связаны между собой заданием или коллекцией (\texttt{Collection}). Термин \texttt{Collection} происходит из аналогичного определения задания в последней версии Google Trace\cite{google-clusterdata}. \texttt{Execution} же является минимальной единицей планирования и может из себя представлять как простые вычисления, так и сложную программу, которой для запуска нужно одновременно выделить несколько серверов. Структуры данных представлены в таблицах \ref{tab:execution} и \ref{tab:collection}.

\begin{table}[h]
    \centering
    \label{tab:execution_request_fields}
    \begin{tabular}{|l|l|p{8cm}|}
        \hline
        \textbf{Поле} & \textbf{Тип} & \textbf{Описание} \\
        \hline
        id & \texttt{u64} & Уникальный идентификатор задачи на всю симуляцию \\
        \hline
        collection\_id & \texttt{Option<u64>} & Индекс задания или коллекции задач  \\
        \hline
        execution\_index & \texttt{Option<u64>} & Индекс задачи внутри задания/коллекции \\
        \hline
        time & \texttt{f64} & Время, когда задача становится доступной для планирования \\
        \hline
        schedule\_after & \texttt{Option<f64>} & Время, когда задача становится доступной для исполнения на кластере. Поле введено для поддержки совместимости с трейсом нагрузки из Google. \\
        \hline
        resources & \texttt{ResourceRequirements} & Структура требуемых ресурсов для исполнения задачи \\
        \hline
        profile & \texttt{Rc<dyn ExecutionProfile>} & Профиль нагрузки (см секцию \ref{sec:execution-profile}) \\
        \hline
        wall\_time\_limit & \texttt{Option<f64>} & Максимальное время на выполнение задачи, после которой она автоматически будет отменена \\
        \hline
        priority & \texttt{Option<u64>} & Приоритет задачи \\
        \hline
        start\_after & \texttt{Option<Vec<u64>>} & Список индексов задач внутри коллекции, после которых можно начать выполнение этой задачи (должен быть указан \texttt{collection\_id}) \\
        \hline
    \end{tabular}
    \caption{Описание структуры данных \texttt{ExecutionRequest}}
    \label{tab:execution}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|p{8cm}|}
        \hline
        \textbf{Поле} & \textbf{Тип} & \textbf{Описание} \\ 
        \hline
        id & \texttt{u64} & Уникальный идентификатор задания/коллекции на всю симуляцию. \\
        \hline
        time & \texttt{f64} & Время, когда задание/коллекция становится известной планировщику. \\
        \hline
        user & \texttt{Option<String>} & Пользователь, который запускает эту коллекцию/задание. Используется для тестирования алгоритмов честного разделения кластера. \\
        \hline
        priority & \texttt{Option<u64>} & Приоритет приоритет задания/коллекции. Если у задачи указан собственный приоритет, то предполагается, что он переопределяет данный приоритет. \\
        \hline
        start\_after\_collection\_ids & \texttt{Option<Vec<u64>>} & Список уникальных идентификаторов заданий/коллекций, после которых можно начать выполнение задач этой коллекции. \\
        \hline
    \end{tabular}
    \caption{Описание структуры данных \texttt{CollectionRequest}}
    \label{tab:collection}
\end{table}

Описание требуемых ресурсов задачи состоит из требования к количеству хостов, а так же опционально к количеству ядер процессора и памяти на хостах. Таким образом, сохраняется возможность поддержать модель планирования аналогичной \texttt{BatSim}, \texttt{IRMaSim} и \texttt{Elastisim}, но и не упускается возможность для моделирования упаковки задач на кластерах (при указывании количества хостов больше одного,  есть возможность указать требования к ресурсам каждого хоста отдельно или всем сразу).

\subsubsection{ExecutionProfile}\label{sec:execution-profile}

Задачи, которые непосредственно выполняются на кластере, представляются в виде объектов, для которых реализован интерфейс \texttt{ExecutionProfile}:

\begin{figure}[h!]
    \small
\begin{rustcode}
#[async_trait(?Send)]
pub trait ExecutionProfile {
  async fn run(self: Rc<Self>, processes: &Vec<HostProcessInstance>);
}
\end{rustcode}
\caption{Интерфейс \texttt{ExecutionProfile}}
\end{figure}

Метод \texttt{run} может быть реализован самим пользователем, но так же могут использоваться заготовки для простых сценариев работы. Такая задача получает в свое <<владение>> от планировщика набор выделенных аллокаций на узлах кластера и выполняет некоторую работу. Сама работа описывается в терминах асинхронного программирования на Rust и использует предоставленный интерфейс взаимодействия с кластером: 
\begin{figure}[h!]
    \footnotesize
\begin{rustcode}
async fn sleep(&self, time: f64);
async fn run_compute_work(&self, compute_work: f64);
async fn transfer_data_process(&self, size: f64, dst_process: ProcessId);
async fn transfer_data_component(&self, size: f64, dst_component: Id);
async fn write_data(&self, size: u64);
async fn read_data(&self, size: u64);
\end{rustcode}
\caption{Интерфейс взаимодействия задачи с кластером}
\end{figure}

Поскольку симулятор поддерживает несколько моделей вычислений, то интерфейс \\ \texttt{ExecutionProfile} принимает на вход несколько процессов (гарантированно хотя бы один). Для простых ситуаций (например, симуляции нагрузки по Google Trace) используются тривиальные имплементации \texttt{ExecutionProfile}, которые рассчитаны на выполнение на одном сервере. 

Для более сложного моделирования работы задачи можно использовать весь спектр асинхронных инструментов из библиотеки \texttt{futures}\cite{rust-futures}. Например, паттерн работы <<Master-Workers>> можно реализовать как показано на примере \ref{fig:execution-profile-impl}.

\begin{figure}[h!]
    \footnotesize 
    \begin{rustcode}
pub struct MasterWorkersProfile {
  pub master_compute_work: f64,
  pub worker_compute_work: f64,
  pub data_transfer_size: f64,
}

#[async_trait(?Send)]
impl ExecutionProfile for MasterWorkersProfile {
  async fn run(self: Rc<Self>, processes: &Vec<HostProcessInstance>) {
    let master = &processes[0];
    let workers = &processes[1..];
    
    futures::future::join_all(workers.iter().map(|p| async {
      master.transfer_data_process(self.data_transfer_size, p.id).await;
      p.run_compute(self.worker_compute_work).await;
    }))
    .await;
    
    master.run_compute(self.master_compute_work).await;
  }
}
    \end{rustcode}
    \caption{Пример реализации интерфейса \texttt{ExecutionProfile}}
    \label{fig:execution-profile-impl}
\end{figure}

\subsubsection{Описание нагрузки через входной файл}

Пользователь может самостоятельно реализовывать свои паттерны нагрузки на кластер, тем самым получая высокую степень точности симуляции. Для использования этих профилей из необходимо зарегистрировать в симуляции перед ее началом, и тогда можно будет подавать на вход задачи, использующие этот профиль. Например, после регистрации профиля \texttt{MasterWorkersProfile} можно описать задачу (\texttt{ExecutionRequest}) следующим образом c помощью YAML-файла: 

\begin{figure}[h!]
    \footnotesize
    \begin{yamlcode}
                    - submit_time: 10.0
                      resources:
                        nodes_count: 1
                        cpu_per_node: 2
                        memory_per_node: 4
                      profile:
                        type: master-workers
                        args:
                          master_compute_work: 100. 
                          worker_compute_work: 2000. 
                          data_transfer_size: 100.
\end{yamlcode}
    \caption{Пример описания задачи с использованием профиля \texttt{MasterWorkersProfile}}
\end{figure}

Во время симуляции будет создан объект задачи с переданными параметрами (из секции \texttt{args}) и передан в планировщик для размещения на кластере. Как только планировщик найдет для задачи место, будет выполнен метод \texttt{run} из реализации \texttt{MasterWorkersProfile} (см. рисунок \ref{fig:execution-profile-impl}). 

\subsubsection{Декларативное создание профилей нагрузки через файл}

Помимо реализации интерфейса \texttt{ExecutionProfile}, есть и другой способ описания профилей нагрузки -- через YAML-файл. Такой способ описания позволяет быстрее создавать новые профили нагрузки без необходимости писать код, но с меньшими функциональными возможностями. Пользователю предоставляется два основных комбинатора профилей: параллельный и последовательный, которые могут быть вложены друг в друга. Пример описания профиля нагрузки представлен на рисунке \ref{fig:profile-yaml}.

\begin{figure}[h!]
    \footnotesize
\begin{subfigure}{0.5\textwidth}
    \begin{yamlcode}
    compute-simple: 
      type: compute-homogenous
      args: 
        compute_work: 1000.0 
    compute-hard:
      type: compute-homogenous
      args:
        compute_work: 2000.0
    net-all-to-all-simple:
      type: communication-homogenous
      args: 
        size: 500.0 
    disk-simple-read: 
      type: disk-read 
      args: 
        size: 1000
    disk-simple-write-part: 
      type: disk-write 
      args: 
        size: 600  
    \end{yamlcode}
    \caption{Пример описания базовых профилей}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
    \begin{yamlcode}
complex_execution_profile:
  type: sequence
  args:
    repeat: 2
    profiles:
      - disk-simple-read
      - compute-simple
      - parallel
        args:
          profiles:
            - compute-hard
            - net-all-to-all-simple
      - disk-simple-write-part
      - parallel
        args:
          profiles:
            - compute-simple
            - net-all-to-all-simple
      - disk-simple-write-part
\end{yamlcode}
    \caption{Пример описания профиля через комбинаторы}
\end{subfigure}
\caption{Пример описания профилей нагрузки через YAML-файл}
\label{fig:profile-yaml}
\end{figure}

Можно видеть, что комбинаторы могут быть вложены, позволяя создавать сложные сценарии. По умолчанию предоставляются базовые профили, описанные в таблице \ref{tab:default-profiles}. Эти профили покрывают функциональность профилей из \texttt{BatSim}\cite{batsim-profile-types-overview} и позволяют создавать разнообразные сценарии нагрузки на кластер, кроме этого с помощью комбинаторов можно сделать более сложные вложенные профили.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|p{7cm}|}
        \hline
        \textbf{Профиль} & \textbf{Аргументы} & \textbf{Описание} \\ 
        \hline
        compute-homogenous & \texttt{compute\_work: f64} & Выполнить \texttt{compute\_work} работы на всех переданных процессах.\\
        \hline
        compute-matrix &  \texttt{matrix: Vec<Vec<f64>>} & Выполнить вычисления на процессах согласно матрице \texttt{matrix}, например: 
        [10, 20] -- первый процесс выполняет работу 10, а второй -- 20. \\
        \hline
        \makecell{
        communication-\\homogenous} & \texttt{size: f64} & Передать \texttt{size} данных между всеми переданными процессами. \\
        \hline 
        communication-matrix &  \texttt{matrix: Vec<Vec<f64>>}  & Передать данные между процессами согласно матрице \texttt{matrix}, например: 
        [0, 20,
        10, 0] -- первый процесс передает второму данные размера 20, а второй первому -- 10. Аналогично с определением матрицы передачи данных из \texttt{BatSim}\cite{batsim-profile-types-overview}. \\
        \hline
        communication-src-dst & \makecell[tl]{
              \texttt{src: Vec<usize>} \\ \texttt{dst: Vec<usize>} \\ \texttt{size: f64} }
         & Передать \texttt{size} данных от процессов с индексами из \texttt{src} к процессам с индексами из \texttt{dst}. \\
        \hline
        \makecell{communication-\\external} & \makecell[tl]{
             \texttt{processes: Vec<usize>}\\ \texttt{input\_size: f64}\\ \texttt{output\_size: f64}\\ \texttt{component\_name: String}} & Передать данные между процессами и другими компонентами сети, например, между процессами и внешним хранилищем. Для этого внешнее хранилище должно быть предварительно зарегистрировано в сети и настроено. \\
            \hline
        disk-read & \makecell[tl]{ \texttt{processes: Vec<usize>}\\ \texttt{size: f64}} & Прочитать \texttt{size} данных с диска у данных процессов. Если процессы не указаны, чтение просиходит у всех. \\
        \hline
        disk-write & \makecell[tl]{\texttt{processes: Vec<usize>}\\\texttt{size: f64}}  & Записать \texttt{size} данных на диск у данных процессов. Если процессы не указаны, запись на диск происходит у всех. \\
        \hline
    \end{tabular}
    \caption{Описание предоставляемых по умолчанию профилей нагрузки}
    \label{tab:default-profiles}
\end{table}

\subsection{Модель планирования заданий}

Симулятор поддерживает несколько основных моделей планирования: 
\begin{enumerate}
    \item Планировщик принимает решение о планировании на основе всей информации (выбирает подходящую задачу и подходящий хост). В таком случае планировщик может вызываться периодически и по событиям (например, по получению новых задач и завершению прошлых). В такой модели у планировщика есть набор задач, которые в данный момент наиболее приоритетны (обычно это некоторая очередь), и планировщик выбирает под эти задачи наиболее подходящие серверы (или части серверов) кластера. Такая модель используется в одном из наиболее популярных планировщиков для кластеров \texttt{Slurm}\cite{slurm-cpu-management}(Simple Linux Utility for Resource Management). В этой модели под задание (job) с помощью отдельного плагина \texttt{Node Selection} выбирается набор серверов, на которые впоследствии распределяются задачи (tasks). 
    \item Вычислительные серверы периодически сообщают планировщику о своих свободных ресурсах, и планировщик принимает решение о подходящих задачах (которые могут аналогичным образом браться из очереди) конкретно под сервер, который сделал запрос. Так же серверы информируют планировщик об изменении свободных ресурсов в связи с завершением текущих задач. Такая модель используется в планировщике кластера YT в Яндексе\cite{yt-scheduler}.
\end{enumerate}

Поддерживая обе эти модели получается, что симулятор способен воспроизводить работу разных планировщиков, что делает его более универсальным. Однако, из-за этого размывается четкое разделение рабочей нагрузки на задание (job) и задачи (tasks). В такой ситуации получается, что \texttt{Execution} (см. раздел \ref{sec:execution-collection-model}) может выступать как простейшая задача продолжительностью несколько минут (из модели планировщика \texttt{YT}, \texttt{Google} и др.), так и сложная много-серверная программа, внутри которой есть своя собственная логика разделения ресурсов (из модели планировщика \texttt{Slurm}). В последнем случае дополнительную логику <<планирования>> и разделения большого объема выделенных ресурсов реализует как раз сам пользователь внутри \texttt{ExecutionProfile}, эта деталь отличает симулятор от планировщика \texttt{Slurm}, где такая логика реализована внутри главного планировщика.


Реализуя свой планировщик пользователь может реализовать интерфейс планировщика \texttt{Scheduler} (см. рисунок \ref{fig:scheduler-interface}). 

\begin{figure}[h!]
    \footnotesize
    \begin{rustcode}
pub trait Scheduler {
  fn on_host_added(&mut self, ctx: &Context, host: HostConfig);
  fn on_execution_request(&mut self, ctx: &Context, req: ExecutionRequest);
  fn on_collection_request(&mut self, ctx: &Context, req: CollectionRequest);
  fn on_execution_finished(&mut self, ctx: &Context, execution_id: u64);
  fn on_host_notification(&mut self, ctx: &Context, host_id: Id, resources: ResourcesPack);
}
    \end{rustcode}
    \caption{Интерфейс планировщика \texttt{Scheduler}}
    \label{fig:scheduler-interface}
\end{figure}

Для более точного управления моделью планировщика можно запустить симуляцию, передав ей компонент отдельный симуляции \texttt{DSLab}, который будет получать сырые события от компонент \texttt{Proxy} и \texttt{Cluster}. Тогда у пользователя будет еще больше контроля над моделью планирования: компонент сможет отправлять себе произвольные события, использовать асинхронные функции из асинхронного расширения ядра \texttt{DSLab} и т.д.

С помощью контекста (который передается в каждый метод интерфейса) планировщик управляет симуляцией: иметь доступ к генератору псевдослучайных чисел, ставить на исполнение задачи, выбрав для них серера, а так же частично отменять их. Полная отмена в данный момент не полностью реализована, например невозможно отменить операцию передачи данных по сети, это связано с внутренними ограничениями  модуля \texttt{dslab\_network}, которые в будущем могут быть доработаны.

\subsection{Входные данные симуляции}

\subsubsection{Трейс нагрузки}\label{sec:input-format-trace}

В качестве входных данных симуляции выступает трейс нагрузки на кластер. Трейс представляет собой набор задач, которые поступают на кластер в течение симуляции. Как описывалось в разделе \ref{sec:execution-collection-model}, они могут быть связаны некоторым общим заданием/коллекцией. В данный момент симулятор поддерживает несколько видов входной нагрузки: 

\begin{itemize}
   \item \texttt{Native} -- трейса нагрузки в формате \texttt{YAML}, описанного в секции \ref{compute-model}. Состоит из нескольких файлов: файл с описанием задач (\texttt{ExecutionRequest}-ы), файл с описанием коллекций (\texttt{CollectionRequest}-ы) и файл с описанием профилей нагрузки (\texttt{ExecutionProfile}).
   \item \texttt{Google} -- трейс нагрузки кластера \texttt{Google}\cite{google-clusterdata}. Версия 2019 года представлена в виде таблицы \texttt{BigQuery}, из которой достаточно сложно скачать большой объем данных (из-за ограниченных квот), однако версию 2011 года можно скачать в виде \texttt{CSV}-файла. Подробное описание и сравнение этих трейсов представлено в статье \texttt{Borg: Next Generation}\cite{borg-next-gen} от Google. Для простоты в симуляторе предполагается, что все задачи потребляют все выделенные ресурсы на протяжении всего времени выполнения (заполняются тривиальными заглушками \texttt{ExecutionProfile}), однако в трейсе 2019 года присутствует подробная информация о профиле загрузки задачи, которую можно использовать для более точного моделирования в последующей работе. Сам трейс представлен в виде событий, поэтому его инкрементальное прочтение сделать сложно (для того, чтобы выяснить время выполнение задачи, необходимо прочитать трейс до события окончания задачи), из-за этого с трейсом удобнее работать, преобразовав его в формат \texttt{Native}. Поскольку данные о требованиях к ресурсам задач представлены в нормированном виде (числами с плавающей точкой от 0 до 1, где 1 -- максимальное возможное значение ресурса среди всех доступных серверов), то при чтении этого трейса необходимо указать константу, на которую умножаются эти значения, чтобы получить целочисленные значения CPU и памяти. 
   \item \texttt{Alibaba} -- Трейс нагрузки кластера \texttt{Alibaba} 2018 года\cite{alibaba-clusterdata}. Состоит из двух файлов: файл \texttt{batch\_tasks.csv} описывает зависимости между выполняемыми задачами (\texttt{instances}), информация о которых содержится в \texttt{batch\_instances.csv}. Информация о задачах по аналогии с Google Trace переводится в тривиальные профили нагрузки. Однако, этот трейс можно читать инкрементально, что упрощает работу с ним.  
   \item \texttt{Standard Workload Format Reader} -- Трейс нагрузки в формате \texttt{SWF}\cite{standard-workload-format}. Транслируется в симулятор по аналогии с Alibaba Trace, т.к. может быть в исходном виде прочитан инкрементально. 
   \item \texttt{Random Trace Generator}. Детерминированный генератор синтетической нагрузки на кластер. С помощью встроенного в симуляцию \texttt{DSLab} генератора случайных чисел создает заданное количество задач с задаными требованями к ресурсам. Время выполнение (либо необходимая процессорная работа) генерируется из нормального распределения с заданными параметрами среднего и дисперсии (см. пример в секции \ref{sec:example-fairness}).

\end{itemize}

Все перечисленные генераторы нагрузки реализуют интерфейс \texttt{WorkloadGenerator} (см. \ref{fig:workload-generator-trait}). Поскольку количество задач может быть очень большим (входные файлы могут иметь размеры десятки гигабайт) и не поместится в оперативную память сразу, то генераторы нагрузки должны уметь читать данные инкрементально, выдавая задачи для планировщика пачками не больше указанного лимита. Предполагается, что задачи в трейсе упорядочены по времени поступления на кластер. 
\begin{figure}[h!]
    \footnotesize
\begin{rustcode}
pub trait WorkloadGenerator {
  fn get_workload(
    &mut self, ctx: &SimulationContext, limit: Option<u64>,
  ) -> Vec<ExecutionRequest>;
  
  fn get_collections(&self, ctx: &SimulationContext) -> Vec<CollectionRequest>;
}
\end{rustcode}
    \caption{Интерфейс \texttt{WorkloadGenerator}}
    \label{fig:workload-generator-trait}
\end{figure}

Пользователь может реализовать этот интерфейс самостоятельно и тогда у него появится возможность использовать свой собственный формат трейса нагрузки. Как альтернативный вариант, можно конвертировать имеющийся трейс в один из поддерживаемых форматов.

\subsubsection{Конфигурация кластера и сети}

Существует несколько способов задать конфигурацию серверов в кластере: 
\begin{enumerate}
    \item Через конфигурационный файл в формате YAML. Пример конфигурации представлен на рисунке \ref{fig:example-sim-config}. Есть возможность задать группы серверов с одинаковой конфигурацией, а затем получать статистику утилизации ресурсов в серверах по группам (подробнее см. раздел \ref{sec:output-monitoring}). Составные части каждого сервера описаны в разделе \ref{cluster-model}. Через конфигурацию так же можно указать вид сети, используемой в кластере, и ее параметры (например, указать распределение хостов по стойкам при использовании топологии \texttt{Fat-Tree}) (описана в разделе \ref{sec:network-model}).
    \item Через чтение конфигурации кластера из \texttt{Google-trace}.
    \item Через чтение конфигурации кластера из \texttt{Alibaba-trace}.
\end{enumerate}

Стоит отметить, что использование трейса нагрузки из какого-либо источника (\texttt{Google trace} или \texttt{Alibaba trace}) не обязывает использовать конфигурацию кластера из этого же источника и наоборот. Однако, учитывая, что в обоих трейсах информация о ресурсах является нормированной (в целях скрыть данные о реальных мощностях кластера и объеме выполняемых задач), использовать их как-то иначе представляется практически невозможным. 

\subsection{Выходные данные симуляции}\label{sec:output-monitoring}

Выходными данными симуляции являются файлы с метриками загруженности кластера и очереди задач. В процессе работы отдельный компонент \texttt{Monitoring} собирает данные о мгновенной загрузке серверов и размере очереди, и агрегирует их, считая среднее значение на заданных временных промежутках. Пользователь может настроить интервал сжатия данных в конфигурационном файле симуляции (см. конфигурацию \ref{fig:example-sim-config}). В разделе честности распределение ресурсов по умолчанию собираются метрики использования доминантной доли ресурса различными пользователями\cite{drf-article}. Подробнее это рассмотрено в примере использования симулятора \ref{sec:example-fairness}.

\subsection{Конфигурация симуляции}

Вся настройка параметров симуляции происходит через конфигурационный файл в формате YAML. Пример конфигурации представлен на рисунке \ref{fig:example-sim-config}. 

\begin{figure}[h!]
    \footnotesize
    \begin{yamlcode}
                    workload:
                      - type: Native
                        path: /path/to/trace
                        options: 
                          profiles_path: /path/to/profiles
                    
                    hosts: 
                      - count: 5
                        cpus: 20
                        memory: 20
                        name_prefix: c
                      - count: 5
                        cpus: 5
                        memory: 60
                        name_prefix: m
                    
                    monitoring:
                      host_load_compression_time_interval: 100
                      scheduler_queue_compression_time_interval: 100
                      display_host_load: false 
                      collect_user_queues: true
                    
                    scheduler:
                      hosts_invoke_interval: 10
\end{yamlcode}
\caption{Пример конфигурации симуляции}
\label{fig:example-sim-config}
\end{figure}

В такой симуляции входная нагрузка будет в формате \texttt{Native} с указанными дополнительными профилями нагрузки. Используются два типа серверов: 5 серверов с 20 ядрами и 20 Гб памяти и 5 серверов с 5 ядрами и 60 Гб памяти. Сбор статистики загруженности кластера ведется только по группам и целиком на кластер, а так же собираются метрики честности планирования (по пользователям). Метрики загруженности и размера очереди сжимаются на отрезках времени в 100 единиц и берется среднее на каждом отрезке. Каждый хост сообщает о свободных ресурсах планировщику каждые 10 единиц времени (единицы измерения могут быть любыми, но по умолчанию подразумеваются секунды). 


\section{Эксперименты и тестирование}\label{sec:experiments}

\subsection{Производительность}

В этом разделе будет проведен замер производительности при симуляции Alibaba Trace на 8 дней (входной файл 120GB). TODO 

\subsection{Честность}\label{sec:example-fairness}

В качестве тестирования симулятора были проведены эксперименты на известный алгоритм честного распределения кластера между несколькими пользователями. Были реализованы алгоритмы \texttt{DRF}\cite{drf-article} и \texttt{Tetris}\cite{tetris-article}. 

Алгоритм DRF (Dominant Resource Fairness) основан на понятии доминантного ресурса пользователя. Если пользователь потребляет 1/10 всей памяти в кластере, 1/15 всех ядер процессора, то для него доминантной долей будет считаться число 1/10 (в ситуации, когда распределяется два доступных ресурса -- CPU и RAM). Смысл алгоритма состоит в том, чтобы каждый пользователь получал равную доминантную долю ресурсов (в случае, когда квоты пользователей равны, иначе доля считается от выделенной на кластере квоты). Поэтому из всех задач, которые готовы к исполнению планировщик в каждый момент времени выбирает самого обделенного пользователя и запускает его задачу. 

Такой подход позволяет обеспечить честное распределение ресурсов, однако упаковка задач на серверах может быть неоптимальной. Для борьбы с этим был предложен алгоритм \texttt{Tetris}\cite{tetris-article}. На вход принимается произвольная функция честности (в нашем случае это DRF), а также <<коэффициент честности>> $f \in [0, 1]$. В каждый момент принятия решения, планировщик сортирует задачи по приоритету их размещения на кластере с точки зрения честности, а затем выбирает наилучшую задачу с точки зрения упаковки среди $(1 - f)$ доли очереди. В качестве эвристической функции упаковки \texttt{Tetris} использует скалярное произведение вектора ресурсов задачи на вектор свободных ресурсов сервера. Чем оно больше, тем лучше задача упаковывается на сервер. В таком виде приоритет явно отдается более тяжелым задачам, чтобы этого избежать, можно разделить результат на нормы векторов ресурсов. 

Таким образом, конфигурируя алгоритм через параметр $f$, можно добиться баланса между честностью и плотностью упаковки задач. При $f = 0$ на каждом шаге планировщик выбирает наиболее подходящую задачу с точки зрения упаковки (т.к. выбирает среди всей доступной очердеи), а при $f = 1$ выбирает наиболее обделенного пользователя (т.к. может взять только первую задачу из очереди). Стоит отметить, что при любых значениях $f < 1$ честность распределения ресурсов не гарантируется и существует возможность развития событий, при котором один из пользователей оказывается полностью обделен ресурсами на кластере, т.к. его задачи всегда подходят хуже других. На практике (при значении $f$ достаточно близком к 1) такое случается крайне редко из-за очень большого количества задач с различными требованиями. 

Для такой симуляции был выбран кластер с распределением ресурсов по серверам, указанными на графике \ref{fig:cluster-resources-distribution}.

\begin{figure}[h!]
    \centering 
    \includegraphics[width=0.7\linewidth]{images/hosts_distribution}
    \caption{Распределение ресурсов на серверах кластера}
    \label{fig:cluster-resources-distribution}
\end{figure}

В качестве синтетической нагрузки были сгенерированы 10 пользователей с равной долей доступа к кластеру с запросами на задачи, представленными в таблице \ref{tab:tasks_load_description}. Каждый пользователь непрерывно поставлял задачи с требованиями, указанными в таблице. Продолжительность задачи генерировалась из нормального распределения с параметрами среднего (\texttt{duration mean}) и дисперсии (\texttt{duration dev}) для каждого пользователя. 

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Пользователь} & \textbf{CPU} & \textbf{RAM} & \textbf{Количество задач} & \textbf{Duration Mean} & \textbf{Duration Dev} \\
        \hline
        user0 & 1 & 5 & 1500 & 250 & 40 \\
        \hline
        user1 & 1 & 10 & 800 & 400 & 40 \\
        \hline
        user2 & 1 & 15 & 800 & 300 & 30 \\
        \hline
        user3 & 2 & 5 & 1300 & 250 & 30 \\
        \hline
        user4 & 2 & 6 & 1200 & 250 & 30 \\
        \hline
        user5 & 2 & 7 & 800 & 300 & 40 \\
        \hline
        user6 & 5 & 4 & 1000 & 180 & 30 \\
        \hline
        user7 & 5 & 5 & 700 & 160 & 10 \\
        \hline
        user8 & 7 & 5 & 200 & 800 & 200 \\
        \hline
        user9 & 10 & 2 & 400 & 220 & 30 \\
        \hline
    \end{tabular}
    \caption{Описание синтетической нагрузки на кластер}
    \label{tab:tasks_load_description}
\end{table}


При симуляции с разными параметрами честности производились замеры средней утилизации кластера от момента полной загрузки кластера до момента, когда у какого-либо пользователя заканчивались задачи. Результаты представлены на графиках \ref{fig:utilization-results}.

\begin{figure}[h!]
    \centering 
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/cpu_utilization}
        \caption{Утилизация CPU в зависимости от $f$}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/memory_utilization}
        \caption{Утилизация памяти в зависимости от $f$}
    \end{subfigure}
    \caption{Утилизация кластера в зависимости от параметра честности}
    \label{fig:utilization-results}
\end{figure}

Из графиков видно, что качественный скачок в утилизации происходит при значении параметра честности $f = 0.8$. Это можно объяснить тем, что ровно при этом значении параметра, алгоритм \texttt{Tetris} получает возможность выбирать из двух задач при планировании (т.к. всего пользователей 10). Как только появляется такая возможность, качество упаковки задач на кластере резко возрастает. 

Чтобы сравнить честность распределения, посмотрим на графики \ref{fig:fairness-results} доминантных долей пользователей в течение времени симуляции. Можно отметить, что при коэффициенте $f = 0.6$ (график \ref{fig:fair_share_06}) честность страдает не сильно, особенно пока у всех пользователей есть задачи, а при $f = 0.5$ (график \ref{fig:fair_share_05}) уже можно видеть, что два пользователя существенно обделяются (получают долю в два раза меньше, чем получает большинство), еще два других пользователя обделяются не так сильно.  На обоих графиках видно, что самым обделенным пользователем является \texttt{user9} -- это можно объяснить тем, что у него профиль нагрузки задач отличается больше всех от других: он потребляет больше всех \texttt{CPU} и меньше всех \texttt{RAM}, поэтому его задачи хуже всего подходят в освободившиеся слоты от других пользователей. 


\begin{figure}[h!]
    \centering 
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/fair_share_0}
        \caption{$f = 0.0$}
        \label{fig:fair_share_0}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/fair_share_1}
        \caption{$f = 1.0$}
        \label{fig:fair_share_1}
    \end{subfigure}

    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/fair_share_05}
        \caption{$f = 0.5$}
        \label{fig:fair_share_05}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/fair_share_06}
        \caption{$f = 0.6$}
        \label{fig:fair_share_06}
    \end{subfigure}
    \caption{Распределение доминантных долей использования кластера пользователями при различных коэффициентах честности $f$}
    \label{fig:fairness-results}
\end{figure}

Ожидаемо можно отметить, что при $f = 1$ доли у всех пользователей абсолютно одинаковые. В случае, когда у какого-то пользователя заканчиваются задачи, освободившиеся ресурсы равномерно распределяются по оставшимся пользователям. Но по времени симуляции, и по утилизации (графики \ref{fig:utilization-results}) видно, что это не самый лучший вариант занять кластер.

Таким образом можно сделать вывод, что для такой конфигурации кластера и такой нагрузки оптимальный параметр честности для классического алгоритма \texttt{Tetris} будет $f = 0.6$. Именно с проверки подобных гипотез можно использовать симулятор вместо того, чтобы запускать все эти эксперименты на реальном кластере, т.к. для проведения подобного эксперимента в симуляторе потребуется несколько секунд времени. 

\subsection{Детерминированность симуляции}

Важным аспектом любой симуляции является воспроизводимость результатов. Фреймворк \texttt{DSLab} предоставляет универсальный интерфейс доступа к генератору случайных чисел в симуляции, который зависит только от переданного в симуляцию сида. Реализованный симулятор полностью использует эту возможность и является детерминированным при условии детерминированности алгоритма планирования. Очень важным является использование генератора случайных чисел из переданного в планировщик контекста (см. интерфейс \ref{fig:scheduler-interface}), однако в стандартных структурах языка \texttt{Rust} используется другой генератор случайных чисел, и это необходимо учитывать. Например, при итерации по ключам стандартной хэш-таблицы они будут возвращаться в разных порядках в зависимости от запуска программы. 

\section{Заключение}\label{sec:conclusion}

В рамках ВКР удалось реализовать многофукнциональный и производительный симулятор вычислительного кластера. Код симулятора опубликован в открытом доступе на GitHub\cite{dslab-cluster-scheduling-repo}. Как результат работы можно выделить основное достижение: возможность пользователям исключительно гибко (по сравнению со всеми другими симуляциями) настраивать профиль загрузки сервера. Использование асинхронных примитивов языка \texttt{Rust} дает много возможностей для точно описания нагрузки. При этом, поддерживаются и более простые сценарии использования симулятора. 

C помощью симулятора проведены несколько экспериментов, которые показывают его применимость на больших симуляциях и в разных моделях планирования: честного распределения ресурсов между пользователями и  оптимальной упаковки задач на кластере. 


В качестве дальнейшего развития симулятора можно модифицировать модули \texttt{DSLab}, сделав возможной более полную отмену задач (добавить такой функционал в модулях \texttt{dslab-network} и \texttt{dslab-disk}). 


